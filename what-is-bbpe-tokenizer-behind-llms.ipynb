{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a0499a",
   "metadata": {
    "papermill": {
     "duration": 0.015065,
     "end_time": "2024-04-25T20:55:45.197187",
     "exception": false,
     "start_time": "2024-04-25T20:55:45.182122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Introduction <a class=\"anchor\"  id=\"Intro\"></a>\n",
    "\n",
    "Unless you have been living under a rock, I am sure that you have heard, used or even worked on some generative AIs. I have been amazed by the ChatGPT when it was first realsed in 2022. I have been wondering how this thing works, it feels like some dark magic. Over the years, this new AI hype has never trended down, but it has been recognized as the next economic engine for decades to come. \n",
    "\n",
    "If you are someone like me, you might find it exciting to persue a deeper technical understanding of generative AI. This is a huge topic and there are so many different concepts that worth to be studyed and researched. So we will only focus on one concept in this post: the tokenization behind most of the state-of-the-art large langauge models (LLMs). \n",
    "\n",
    "For many LLMs, the quality of the tokenization determines the quality of the training data fed into the neuron network. In many cases, the root causes of many LLMs' issues are related to the tokenization used. So it is important to build a solid konwledge fundation on this topic if we want to further puesue a deeper understanding on the LLMs and other generative AIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3514d7",
   "metadata": {
    "papermill": {
     "duration": 0.013624,
     "end_time": "2024-04-25T20:55:45.224872",
     "exception": false,
     "start_time": "2024-04-25T20:55:45.211248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Objective <a class=\"anchor\"  id=\"objective\"></a>\n",
    "\n",
    "BBPE (Byte Level Byte Pair Encoding) tokenizer was brought into public's attention by OpenAI in their GPT2 paper. It gains more popularity afterwards and it now becomes the standard parctices in the start of art LLMs. So this post, we will go through the following sections that aims to understand how BBPE works, and why they are popular compared to other type of tokenizers. \n",
    "\n",
    "**Table of Contents**\n",
    "* [Project Introduction](#Intro)\n",
    "* [Objective](#objective)\n",
    "* [Why BBPE?](#chapter1)\n",
    "    * [Word-level Tokenizer](#section_1_1)\n",
    "    * [Character-level Tokenizer](#section_1_2)\n",
    "    * [Byte-level Tokenizer](#section_1_3)\n",
    "    * [Advantage of BBPE Tokenizer](#section_1_4)\n",
    "* [Deep dive into BBPE (Byte-level Byte Pair Encoding) Tokenizer](#chapter2)\n",
    "    * [Quick intro to BPE](#section_2_1)\n",
    "    * [BBPE by steps](#section_2_2)\n",
    "        * [Convert texts to raw bytes](#section_2_2_1)\n",
    "        * [Find the most frequent pair in the whole sequence](#section_2_2_2)\n",
    "        * [replace the most frequent pair with new ID](#section_2_2_3)\n",
    "        * [Train a BBPE tokenizer with all steps above](#section_2_2_4)\n",
    "        * [Encoding and Decoding ](#section_2_2_5)\n",
    "* [Conclusion](#chapter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24deba00",
   "metadata": {
    "papermill": {
     "duration": 0.013697,
     "end_time": "2024-04-25T20:55:45.252310",
     "exception": false,
     "start_time": "2024-04-25T20:55:45.238613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why BBPE? <a class=\"anchor\"  id=\"chapter1\"></a>\n",
    "\n",
    "To understand why BBPE could be a popular choice as the tokenizer in many LLMs, we need to get faimilar with other types of tokenizer existed and what the disadvantages of them compared to the BBPE. \n",
    "\n",
    "In this section, we will look into different type of tokenizers and explain why other types are not good fit for the large language models.\n",
    "\n",
    "* Word-level Tokenizer\n",
    "* Character-level Tokenizer\n",
    "* Byte-level Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952db60",
   "metadata": {
    "papermill": {
     "duration": 0.013706,
     "end_time": "2024-04-25T20:55:45.281532",
     "exception": false,
     "start_time": "2024-04-25T20:55:45.267826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Word-level Tokenizer <a class=\"anchor\"  id=\"section_1_1\"></a>\n",
    "\n",
    "Word-level tokenizer will divide a sentense into words or sub-words based on certain rules. For example, spliting a sentence \"Dogs are cute.\" with the spaces between and we can get [\"Dogs\", \"are\", \"cute.\"]. And then each of those words or sub-words can be encoded into integers which can be used for model training. \n",
    "\n",
    "Let's go through a example and discuss the potential issue with this when training a large language model with massive amount of corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c35411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:45.311503Z",
     "iopub.status.busy": "2024-04-25T20:55:45.311069Z",
     "iopub.status.idle": "2024-04-25T20:55:52.631884Z",
     "shell.execute_reply": "2024-04-25T20:55:52.630677Z"
    },
    "papermill": {
     "duration": 7.339038,
     "end_time": "2024-04-25T20:55:52.634848",
     "exception": false,
     "start_time": "2024-04-25T20:55:45.295810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. word level tokenization\n",
    "import spacy\n",
    "word_tokenizer = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f839d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:52.665425Z",
     "iopub.status.busy": "2024-04-25T20:55:52.664104Z",
     "iopub.status.idle": "2024-04-25T20:55:52.710722Z",
     "shell.execute_reply": "2024-04-25T20:55:52.709760Z"
    },
    "papermill": {
     "duration": 0.0644,
     "end_time": "2024-04-25T20:55:52.713279",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.648879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hello, ,, how, are, you, doing, today, ðŸ‘‹, ?]\n",
      "number of tokens encoded: 9\n"
     ]
    }
   ],
   "source": [
    "eng_text = 'Hello, how are you doing todayðŸ‘‹?'\n",
    "print(list(word_tokenizer(eng_text)))\n",
    "print(f\"number of tokens encoded: {len(word_tokenizer(eng_text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f23f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:52.743111Z",
     "iopub.status.busy": "2024-04-25T20:55:52.742669Z",
     "iopub.status.idle": "2024-04-25T20:55:52.766176Z",
     "shell.execute_reply": "2024-04-25T20:55:52.764989Z"
    },
    "papermill": {
     "duration": 0.041595,
     "end_time": "2024-04-25T20:55:52.768817",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.727222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ä½ å¥½ï¼Œä½ ä»Šå¤©è¿‡çš„æ€Žä¹ˆæ ·, ðŸ‘‹, ï¼Ÿ]\n",
      "number of tokens encoded: 3\n"
     ]
    }
   ],
   "source": [
    "chinese_text = 'ä½ å¥½ï¼Œä½ ä»Šå¤©è¿‡çš„æ€Žä¹ˆæ ·ðŸ‘‹ï¼Ÿ'\n",
    "\n",
    "print(list(word_tokenizer(chinese_text))) \n",
    "print(f\"number of tokens encoded: {len(word_tokenizer(chinese_text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7cd634",
   "metadata": {
    "papermill": {
     "duration": 0.013581,
     "end_time": "2024-04-25T20:55:52.796374",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.782793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the first English text, the word tokenizer works pretty well. It recognized every word and seperate the text into small sections. \n",
    "\n",
    "However, when I provide the Chinese version of the text, the tokenizer could not recognize the Chinese characters and we have all Chinese characters combined into one token.\n",
    "\n",
    "The issue is that we only imported the English vocabulary, so the tokenizer cannot recognize other language. In order to create a tokenizer that can work for all languages around the world, we will have to include all possible words in the vocabulary. Not only that, we will have to include all different version of the same characters, such as \"cat\", \"cats\", \"Cat\"...\n",
    "\n",
    "We can only image how big the size of the vocabulary will be for word-level tokenizer. Just to give a perspective, TransformerXL is a word-level tokenizer and its vocabulary size is 267,735."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a316f91",
   "metadata": {
    "papermill": {
     "duration": 0.013537,
     "end_time": "2024-04-25T20:55:52.823994",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.810457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Character-level Tokenizer <a class=\"anchor\"  id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a808ec",
   "metadata": {
    "papermill": {
     "duration": 0.013496,
     "end_time": "2024-04-25T20:55:52.851287",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.837791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Other than word level, another common approach is to use Unicode to tokenize text in character level. This type of tokenizer will split the texts into individual characters. For the same example \"Dogs are cute.\", we can split it into a list of characters [\"D\", \"o\", \"g\", \"s\", \" \", \"a\", \"r\", \"e\", \" \", \"c\", \"u\", \"t\", \"e\", \".\"]. \n",
    "\n",
    "In this space, using Unicode is a popular choice because Unicode is a text encoding standard designed to support all writen characters acorss all major languages. We will go through a example using character level tokenizer and we can discuss why they are also not the best option for LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45aa8f33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:52.881767Z",
     "iopub.status.busy": "2024-04-25T20:55:52.881113Z",
     "iopub.status.idle": "2024-04-25T20:55:52.887936Z",
     "shell.execute_reply": "2024-04-25T20:55:52.886725Z"
    },
    "papermill": {
     "duration": 0.024913,
     "end_time": "2024-04-25T20:55:52.890314",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.865401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All characters in the English text: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 't', 'o', 'd', 'a', 'y', 'ðŸ‘‹', '?']\n",
      "\n",
      "Tokens in Unicode: [72, 101, 108, 108, 111, 44, 32, 104, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 32, 100, 111, 105, 110, 103, 32, 116, 111, 100, 97, 121, 128075, 63]\n",
      "\n",
      "number of tokens encoded: 32\n"
     ]
    }
   ],
   "source": [
    "# same example with the first text:\n",
    "print(f\"All characters in the English text: {[*eng_text]}\\n\")\n",
    "print(f\"Tokens in Unicode: {[ord(x) for x in eng_text]}\\n\")\n",
    "print(f\"number of tokens encoded: {len(eng_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7fb830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:52.921218Z",
     "iopub.status.busy": "2024-04-25T20:55:52.920268Z",
     "iopub.status.idle": "2024-04-25T20:55:52.926398Z",
     "shell.execute_reply": "2024-04-25T20:55:52.925474Z"
    },
    "papermill": {
     "duration": 0.02378,
     "end_time": "2024-04-25T20:55:52.928785",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.905005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All characters in the Chinese text: ['ä½ ', 'å¥½', 'ï¼Œ', 'ä½ ', 'ä»Š', 'å¤©', 'è¿‡', 'çš„', 'æ€Ž', 'ä¹ˆ', 'æ ·', 'ðŸ‘‹', 'ï¼Ÿ']\n",
      "\n",
      "Tokens in Unicode: [20320, 22909, 65292, 20320, 20170, 22825, 36807, 30340, 24590, 20040, 26679, 128075, 65311]\n",
      "\n",
      "number of tokens encoded: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"All characters in the Chinese text: {[*chinese_text]}\\n\")\n",
    "print(f\"Tokens in Unicode: {[ord(x) for x in chinese_text]}\\n\")\n",
    "print(f\"number of tokens encoded: {len(chinese_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf115ed1",
   "metadata": {
    "papermill": {
     "duration": 0.013761,
     "end_time": "2024-04-25T20:55:52.956893",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.943132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The Unicode tokenizer seems to work very well for our examples. Since Unicode contains all characters in all languages, the tokenizer is able to conver all texts into different tokens. Notice that the unicode can convert the texts into a list of integers, so in theory, LLMs can use those to train the neurons, but what is the catch? \n",
    "\n",
    "Here are two disadvantages of using this character-level tokenizer naively.\n",
    "\n",
    "1. Unicode contains almost 150K different characters, so the base vocabulary size will be quite large. If we then further combine multiple characters into new tokens, the size of vocabulary will grow even more.\n",
    "\n",
    "2. Unicode is a constantly envolving standard. If we use the unicode as our tokenizer, we will have new characters that our old version of tokenizer cannot recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417387e",
   "metadata": {
    "papermill": {
     "duration": 0.013926,
     "end_time": "2024-04-25T20:55:52.985138",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.971212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Byte-level Tokenizer <a class=\"anchor\"  id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1c39a",
   "metadata": {
    "papermill": {
     "duration": 0.013889,
     "end_time": "2024-04-25T20:55:53.013334",
     "exception": false,
     "start_time": "2024-04-25T20:55:52.999445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "One of the approach to limit the vocabulary size it to use byte level tokenizer. When the tokenizer is byte level. For example, we can encode the unicode sequences we obtained prior to UTF-8, UTF-16 or UTF-32 to obtain byte-level tokens. Since each byte is 8-bits binary numbers, so there are total 2^8 different possibilities for one byte which equals to 256 as vocabulary size.\n",
    "\n",
    "Let's use the English texts as example in following to explore different UTF encoding standards:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f02a46",
   "metadata": {
    "papermill": {
     "duration": 0.013788,
     "end_time": "2024-04-25T20:55:53.041281",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.027493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83186e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.071538Z",
     "iopub.status.busy": "2024-04-25T20:55:53.070862Z",
     "iopub.status.idle": "2024-04-25T20:55:53.078217Z",
     "shell.execute_reply": "2024-04-25T20:55:53.077014Z"
    },
    "papermill": {
     "duration": 0.025189,
     "end_time": "2024-04-25T20:55:53.080732",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.055543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte level of tokens of the text in utf-8: [72, 101, 108, 108, 111, 44, 32, 104, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 32, 100, 111, 105, 110, 103, 32, 116, 111, 100, 97, 121, 240, 159, 145, 139, 63]\n",
      "\n",
      "number of tokens encoded: 35\n"
     ]
    }
   ],
   "source": [
    "# use unicode to represents all characters and then encode it using different UTF standards\n",
    "print(f\"byte level of tokens of the text in utf-8: {list(eng_text.encode('utf-8'))}\\n\")\n",
    "print(f\"number of tokens encoded: {len(list(eng_text.encode('utf-8')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0884d",
   "metadata": {
    "papermill": {
     "duration": 0.01515,
     "end_time": "2024-04-25T20:55:53.111032",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.095882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### UTF-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8d5907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.142756Z",
     "iopub.status.busy": "2024-04-25T20:55:53.141893Z",
     "iopub.status.idle": "2024-04-25T20:55:53.148806Z",
     "shell.execute_reply": "2024-04-25T20:55:53.147595Z"
    },
    "papermill": {
     "duration": 0.025785,
     "end_time": "2024-04-25T20:55:53.151205",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.125420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte level of tokens of the text in utf-16: [255, 254, 72, 0, 101, 0, 108, 0, 108, 0, 111, 0, 44, 0, 32, 0, 104, 0, 111, 0, 119, 0, 32, 0, 97, 0, 114, 0, 101, 0, 32, 0, 121, 0, 111, 0, 117, 0, 32, 0, 100, 0, 111, 0, 105, 0, 110, 0, 103, 0, 32, 0, 116, 0, 111, 0, 100, 0, 97, 0, 121, 0, 61, 216, 75, 220, 63, 0]\n",
      "\n",
      "number of tokens encoded: 68\n"
     ]
    }
   ],
   "source": [
    "# use unicode to represents all characters and then encode it using different UTF standards\n",
    "print(f\"byte level of tokens of the text in utf-16: {list(eng_text.encode('utf-16'))}\\n\")\n",
    "print(f\"number of tokens encoded: {len(list(eng_text.encode('utf-16')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c80375",
   "metadata": {
    "papermill": {
     "duration": 0.015066,
     "end_time": "2024-04-25T20:55:53.181041",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.165975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### UTF-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa64ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.213848Z",
     "iopub.status.busy": "2024-04-25T20:55:53.213136Z",
     "iopub.status.idle": "2024-04-25T20:55:53.218817Z",
     "shell.execute_reply": "2024-04-25T20:55:53.217748Z"
    },
    "papermill": {
     "duration": 0.025368,
     "end_time": "2024-04-25T20:55:53.221569",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.196201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte level of tokens of the text in utf-32: [255, 254, 0, 0, 72, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 44, 0, 0, 0, 32, 0, 0, 0, 104, 0, 0, 0, 111, 0, 0, 0, 119, 0, 0, 0, 32, 0, 0, 0, 97, 0, 0, 0, 114, 0, 0, 0, 101, 0, 0, 0, 32, 0, 0, 0, 121, 0, 0, 0, 111, 0, 0, 0, 117, 0, 0, 0, 32, 0, 0, 0, 100, 0, 0, 0, 111, 0, 0, 0, 105, 0, 0, 0, 110, 0, 0, 0, 103, 0, 0, 0, 32, 0, 0, 0, 116, 0, 0, 0, 111, 0, 0, 0, 100, 0, 0, 0, 97, 0, 0, 0, 121, 0, 0, 0, 75, 244, 1, 0, 63, 0, 0, 0]\n",
      "\n",
      "number of tokens encoded: 132\n"
     ]
    }
   ],
   "source": [
    "# use unicode to represents all characters and then encode it using different UTF standards\n",
    "print(f\"byte level of tokens of the text in utf-32: {list(eng_text.encode('utf-32'))}\\n\")\n",
    "print(f\"number of tokens encoded: {len(list(eng_text.encode('utf-32')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d81935",
   "metadata": {
    "papermill": {
     "duration": 0.014118,
     "end_time": "2024-04-25T20:55:53.250549",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.236431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice that the number of tokens doubles everytime when we switch from UTF-8, UFT-16 and UTF-32. There is advantage of using UTF-32 as encoder since all characters will be encoded to the same length (4 Bytes). But we see there are so many zeros in the encoded tokens, and that is a big waste in memory. \n",
    "\n",
    "Therefore, in current standard practice, UTF-8 is the best choice because it is variable length encoder (1-4 bytes), so it can minimize the number of tokens in the end.\n",
    "\n",
    "### Issues of using unicode naively\n",
    "\n",
    "As we can see from above, when we use UTF-8 to encode our text to byte level, we can obtain a nice list of tokens that can be fed into neuron networks. However, there is a big disadvantage of using that naively as our tokenizer. \n",
    "\n",
    "Since for each byte, there is only 256 possibilities. For a complicated characters, it will be encoded into multiple bytes. Shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a87092a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.282195Z",
     "iopub.status.busy": "2024-04-25T20:55:53.281500Z",
     "iopub.status.idle": "2024-04-25T20:55:53.288218Z",
     "shell.execute_reply": "2024-04-25T20:55:53.287019Z"
    },
    "papermill": {
     "duration": 0.025048,
     "end_time": "2024-04-25T20:55:53.290677",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.265629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English character 'a' is encoded as [97]\n",
      "Chinese character 'é¾™' is encoded as [233, 190, 153]\n"
     ]
    }
   ],
   "source": [
    "print(f\"English character 'a' is encoded as {list('a'.encode('utf-8'))}\")\n",
    "print(f\"Chinese character 'é¾™' is encoded as {list('é¾™'.encode('utf-8'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5772bce",
   "metadata": {
    "papermill": {
     "duration": 0.014642,
     "end_time": "2024-04-25T20:55:53.319916",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.305274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When using UTF-8 to encode our texts into tokens, we will expect a very long sequence of tokens as the trade off of having small base vocabulary. However, transformers used in LLMs can only support certain length of input because transformer has limited attention length in the current level of computation. Having a long sequence will limit the efficiency of the training of transformer. \n",
    "\n",
    "Therefore, a text compression is needed in this case to reduce the token size. Also, we want to adjust the vocabulary size so we can further balance between the vocabulary size and the encoded tokens size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5d2f9",
   "metadata": {
    "papermill": {
     "duration": 0.014708,
     "end_time": "2024-04-25T20:55:53.349560",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.334852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Advantages of BBPE tokenizer <a class=\"anchor\"  id=\"section_1_4\"></a>\n",
    "\n",
    "The idea of using BBPE tokenizer over other kinds are menioned in the GPT2 paper. It aims to solve the issues that we have mentioned above. \n",
    "\n",
    "- First, using byte level tokenizer, we can have a base vocabulary of 256 which is very small size. \n",
    "- Second, by applying BPE algorithm, we can also add new merge rules to our vocabulary to expand the size, so the encoded sequence can be compressed. \n",
    "\n",
    "So by applying BPE in the byte level is a clever way to give us the flexibility of choosing a vocabulary size that can best fit our needs. In the next section, I will go deeper into this tokenizer approach and explain how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a9268",
   "metadata": {
    "papermill": {
     "duration": 0.014464,
     "end_time": "2024-04-25T20:55:53.381577",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.367113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deep dive into BBPE (Byte-level Byte Pair Encoding) Tokenizer <a class=\"anchor\"  id=\"chapter2\"></a>\n",
    "\n",
    "## Quick intro to BPE <a class=\"anchor\"  id=\"section_2_1\"></a>\n",
    "\n",
    "Byte Pair Encoding is a text compression technique that merges commonly appeared character pairs. Here is a quick example (in Wikipedia):\n",
    "\n",
    "1. Given data for encode: **aaabdaaabac**\n",
    "\n",
    "2. Since byte pair 'aa' are the most often occurred, we will create a merge rule by replace all this pair with a new byte that have not included in the data: **Z = aa**. Now, the data becomes: **ZabdZabac**\n",
    "\n",
    "3. Repeat step 2: found the most frequent appearred pair as 'ab', create merge rule: **Y = ab**. Now, the data is **ZYdZYac**.\n",
    "\n",
    "4. Repeat again with new merge rule: **X = ZY**. And data is now **XdXac**. \n",
    "\n",
    "5. There is no pair that occurs more than once in the data. So we will have a compressed version of the data: **XdXac** and we can decode it back to orginal with the three merge rules: **X=ZY; Y=ab; Z=aa**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522ff52",
   "metadata": {
    "papermill": {
     "duration": 0.014674,
     "end_time": "2024-04-25T20:55:53.411529",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.396855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BBPE by steps <a class=\"anchor\"  id=\"section_2_2\"></a>\n",
    "When BPE is operated on byte level, the given text will be encoded in UTF-8 into raw bytes. Next the raw bytes will be used to create merge rules and we will then use the merge rule to compress the previous bytes sequence. After a few iterations, the initial sequences will have a much smaller size. Let's go through the training process one step at a time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8dd18",
   "metadata": {
    "papermill": {
     "duration": 0.014424,
     "end_time": "2024-04-25T20:55:53.440875",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.426451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. Convert texts to raw bytes <a class=\"anchor\"  id=\"section_2_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41d7679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.473212Z",
     "iopub.status.busy": "2024-04-25T20:55:53.471948Z",
     "iopub.status.idle": "2024-04-25T20:55:53.477453Z",
     "shell.execute_reply": "2024-04-25T20:55:53.476423Z"
    },
    "papermill": {
     "duration": 0.024428,
     "end_time": "2024-04-25T20:55:53.479951",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.455523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I took a random paragraph from the BPE Wikipedia page as our example text.\n",
    "example_text = \"The original algorithm operates by iteratively replacing the most common contiguous sequences of characters in a target text with unused 'placeholder' bytes. The iteration ends when no sequences can be found, leaving the target text effectively compressed. Decompression can be performed by reversing this process, querying known placeholder terms against their corresponding denoted sequence, using a lookup table. In the original paper, this lookup table is encoded and stored alongside the compressed text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d509fe0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.512855Z",
     "iopub.status.busy": "2024-04-25T20:55:53.511583Z",
     "iopub.status.idle": "2024-04-25T20:55:53.518440Z",
     "shell.execute_reply": "2024-04-25T20:55:53.517126Z"
    },
    "papermill": {
     "duration": 0.026113,
     "end_time": "2024-04-25T20:55:53.521233",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.495120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UTF-8 to encode the text to raw bytes: [84, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 111, 112, 101, 114, 97, 116, 101, 115, 32, 98, 121, 32, 105, 116, 101, 114, 97, 116, 105, 118, 101, 108, 121, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 99, 111, 109, 109, 111, 110, 32, 99, 111, 110, 116, 105, 103, 117, 111, 117, 115, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 105, 110, 32, 97, 32, 116, 97, 114, 103, 101, 116, 32, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 117, 110, 117, 115, 101, 100, 32, 39, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 39, 32, 98, 121, 116, 101, 115, 46, 32, 84, 104, 101, 32, 105, 116, 101, 114, 97, 116, 105, 111, 110, 32, 101, 110, 100, 115, 32, 119, 104, 101, 110, 32, 110, 111, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 102, 111, 117, 110, 100, 44, 32, 108, 101, 97, 118, 105, 110, 103, 32, 116, 104, 101, 32, 116, 97, 114, 103, 101, 116, 32, 116, 101, 120, 116, 32, 101, 102, 102, 101, 99, 116, 105, 118, 101, 108, 121, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 46, 32, 68, 101, 99, 111, 109, 112, 114, 101, 115, 115, 105, 111, 110, 32, 99, 97, 110, 32, 98, 101, 32, 112, 101, 114, 102, 111, 114, 109, 101, 100, 32, 98, 121, 32, 114, 101, 118, 101, 114, 115, 105, 110, 103, 32, 116, 104, 105, 115, 32, 112, 114, 111, 99, 101, 115, 115, 44, 32, 113, 117, 101, 114, 121, 105, 110, 103, 32, 107, 110, 111, 119, 110, 32, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 32, 116, 101, 114, 109, 115, 32, 97, 103, 97, 105, 110, 115, 116, 32, 116, 104, 101, 105, 114, 32, 99, 111, 114, 114, 101, 115, 112, 111, 110, 100, 105, 110, 103, 32, 100, 101, 110, 111, 116, 101, 100, 32, 115, 101, 113, 117, 101, 110, 99, 101, 44, 32, 117, 115, 105, 110, 103, 32, 97, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 46, 32, 73, 110, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 112, 97, 112, 101, 114, 44, 32, 116, 104, 105, 115, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 32, 105, 115, 32, 101, 110, 99, 111, 100, 101, 100, 32, 97, 110, 100, 32, 115, 116, 111, 114, 101, 100, 32, 97, 108, 111, 110, 103, 115, 105, 100, 101, 32, 116, 104, 101, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 32, 116, 101, 120, 116, 46]\n",
      "\n",
      "the number of tokens is 509\n"
     ]
    }
   ],
   "source": [
    "tokens_in_utf8 = list(example_text.encode(\"utf-8\"))\n",
    "print(f\"Using UTF-8 to encode the text to raw bytes: {tokens_in_utf8}\\n\")\n",
    "print(f\"the number of tokens is {len(tokens_in_utf8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e90162",
   "metadata": {
    "papermill": {
     "duration": 0.014344,
     "end_time": "2024-04-25T20:55:53.550408",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.536064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. find the most frequent pair in the whole sequence <a class=\"anchor\"  id=\"section_2_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19bf90d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.582140Z",
     "iopub.status.busy": "2024-04-25T20:55:53.581702Z",
     "iopub.status.idle": "2024-04-25T20:55:53.590446Z",
     "shell.execute_reply": "2024-04-25T20:55:53.589333Z"
    },
    "papermill": {
     "duration": 0.028133,
     "end_time": "2024-04-25T20:55:53.593837",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.565704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will sort the pair counts in order to find the most frequent pair to merge: \n",
      "{(32, 116): 15, (101, 114): 11, (101, 32): 10, (105, 110): 10, (116, 101): 10, (115, 32): 10, (116, 104): 9, (101, 115): 9, (110, 32): 9, (104, 101): 8, (32, 99): 8, (114, 101): 7, (110, 103): 7, (99, 111): 7, (101, 110): 7, (101, 100): 7, (100, 32): 7, (111, 114): 6, (32, 97): 6, (103, 32): 6, (116, 32): 6, (111, 110): 6, (115, 101): 6, (99, 101): 6, (32, 98): 5, (100, 101): 5, (32, 111): 4, (97, 108): 4, (105, 116): 4, (114, 97): 4, (121, 32): 4, (32, 105): 4, (116, 105): 4, (97, 99): 4, (111, 109): 4, (32, 115): 4, (113, 117): 4, (117, 101): 4, (110, 99): 4, (116, 97): 4, (110, 100): 4, (44, 32): 4, (112, 114): 4, (115, 115): 4, (115, 105): 4, (32, 112): 4, (114, 105): 3, (105, 103): 3, (112, 101): 3, (97, 116): 3, (98, 121): 3, (118, 101): 3, (112, 108): 3, (108, 97): 3, (115, 116): 3, (117, 115): 3, (101, 113): 3, (97, 114): 3, (101, 120): 3, (120, 116): 3, (46, 32): 3, (32, 101): 3, (110, 111): 3, (97, 110): 3, (32, 108): 3, (108, 101): 3, (109, 112): 3, (105, 115): 3, (108, 111): 3, (84, 104): 2, (103, 105): 2, (110, 97): 2, (108, 32): 2, (105, 118): 2, (101, 108): 2, (108, 121): 2, (32, 114): 2, (109, 111): 2, (111, 117): 2, (99, 116): 2, (114, 115): 2, (97, 32): 2, (114, 103): 2, (103, 101): 2, (101, 116): 2, (32, 119): 2, (32, 117): 2, (117, 110): 2, (101, 104): 2, (104, 111): 2, (111, 108): 2, (108, 100): 2, (105, 111): 2, (99, 97): 2, (98, 101): 2, (102, 111): 2, (101, 99): 2, (114, 109): 2, (104, 105): 2, (114, 32): 2, (111, 111): 2, (111, 107): 2, (107, 117): 2, (117, 112): 2, (112, 32): 2, (97, 98): 2, (98, 108): 2, (108, 103): 1, (103, 111): 1, (104, 109): 1, (109, 32): 1, (111, 112): 1, (101, 112): 1, (99, 105): 1, (32, 109): 1, (111, 115): 1, (109, 109): 1, (110, 116): 1, (103, 117): 1, (117, 111): 1, (111, 102): 1, (102, 32): 1, (99, 104): 1, (104, 97): 1, (119, 105): 1, (104, 32): 1, (110, 117): 1, (32, 39): 1, (39, 112): 1, (114, 39): 1, (39, 32): 1, (121, 116): 1, (115, 46): 1, (32, 84): 1, (100, 115): 1, (119, 104): 1, (32, 110): 1, (111, 32): 1, (32, 102): 1, (100, 44): 1, (101, 97): 1, (97, 118): 1, (118, 105): 1, (101, 102): 1, (102, 102): 1, (102, 101): 1, (100, 46): 1, (32, 68): 1, (68, 101): 1, (114, 102): 1, (109, 101): 1, (101, 118): 1, (114, 111): 1, (111, 99): 1, (115, 44): 1, (32, 113): 1, (114, 121): 1, (121, 105): 1, (32, 107): 1, (107, 110): 1, (111, 119): 1, (119, 110): 1, (109, 115): 1, (97, 103): 1, (103, 97): 1, (97, 105): 1, (110, 115): 1, (101, 105): 1, (105, 114): 1, (114, 114): 1, (115, 112): 1, (112, 111): 1, (100, 105): 1, (32, 100): 1, (111, 116): 1, (101, 44): 1, (101, 46): 1, (32, 73): 1, (73, 110): 1, (112, 97): 1, (97, 112): 1, (114, 44): 1, (111, 100): 1, (116, 111): 1, (103, 115): 1, (105, 100): 1, (116, 46): 1}\n",
      "\n",
      "the most frequently appearred pair is (32, 116)\n"
     ]
    }
   ],
   "source": [
    "# First, count the pair frequencies\n",
    "count = {}\n",
    "for i in zip(tokens_in_utf8, tokens_in_utf8[1:]):\n",
    "    if i not in count.keys():\n",
    "        count[i] = 1\n",
    "    else:\n",
    "        count[i] += 1\n",
    "\n",
    "# sort the pair counts to find the most frequent pair\n",
    "sorted_pair_counts = {k: v for k, v in sorted(count.items(), key = lambda item: item[1], reverse = True)}\n",
    "print(f\"we will sort the pair counts in order to find the most frequent pair to merge: \\n{sorted_pair_counts}\\n\")\n",
    "print(f\"the most frequently appearred pair is {list(sorted_pair_counts.keys())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56be15",
   "metadata": {
    "papermill": {
     "duration": 0.014477,
     "end_time": "2024-04-25T20:55:53.623139",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.608662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. replace the most frequent pair with new ID <a class=\"anchor\"  id=\"section_2_2_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0462fd6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.655091Z",
     "iopub.status.busy": "2024-04-25T20:55:53.654655Z",
     "iopub.status.idle": "2024-04-25T20:55:53.663472Z",
     "shell.execute_reply": "2024-04-25T20:55:53.661967Z"
    },
    "papermill": {
     "duration": 0.028102,
     "end_time": "2024-04-25T20:55:53.666079",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.637977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# since we are create our first merge rule, we can set the new id as 256\n",
    "merge_pair = list(sorted_pair_counts.keys())[0]\n",
    "new_tokens = []\n",
    "new_id = 256\n",
    "i = 0\n",
    "\n",
    "while i < len(tokens_in_utf8):\n",
    "    # if we are at the last index, we will break out the loop to aviod error\n",
    "    # then we will match current pair with the merge pair\n",
    "    if (i < len(tokens_in_utf8) - 1) and (tokens_in_utf8[i] == merge_pair[0]) and (tokens_in_utf8[i+1] == merge_pair[1]):\n",
    "        new_tokens.append(new_id)\n",
    "        # if we found a merge pair, add 2 to i to skip next value in list\n",
    "        i += 2\n",
    "    else:\n",
    "        new_tokens.append(tokens_in_utf8[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8555e059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.697861Z",
     "iopub.status.busy": "2024-04-25T20:55:53.697385Z",
     "iopub.status.idle": "2024-04-25T20:55:53.703349Z",
     "shell.execute_reply": "2024-04-25T20:55:53.702127Z"
    },
    "papermill": {
     "duration": 0.025424,
     "end_time": "2024-04-25T20:55:53.706618",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.681194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After one iteration, we got our new list of tokens: [84, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 111, 112, 101, 114, 97, 116, 101, 115, 32, 98, 121, 32, 105, 116, 101, 114, 97, 116, 105, 118, 101, 108, 121, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 256, 104, 101, 32, 109, 111, 115, 116, 32, 99, 111, 109, 109, 111, 110, 32, 99, 111, 110, 116, 105, 103, 117, 111, 117, 115, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 105, 110, 32, 97, 256, 97, 114, 103, 101, 116, 256, 101, 120, 116, 32, 119, 105, 116, 104, 32, 117, 110, 117, 115, 101, 100, 32, 39, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 39, 32, 98, 121, 116, 101, 115, 46, 32, 84, 104, 101, 32, 105, 116, 101, 114, 97, 116, 105, 111, 110, 32, 101, 110, 100, 115, 32, 119, 104, 101, 110, 32, 110, 111, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 102, 111, 117, 110, 100, 44, 32, 108, 101, 97, 118, 105, 110, 103, 256, 104, 101, 256, 97, 114, 103, 101, 116, 256, 101, 120, 116, 32, 101, 102, 102, 101, 99, 116, 105, 118, 101, 108, 121, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 46, 32, 68, 101, 99, 111, 109, 112, 114, 101, 115, 115, 105, 111, 110, 32, 99, 97, 110, 32, 98, 101, 32, 112, 101, 114, 102, 111, 114, 109, 101, 100, 32, 98, 121, 32, 114, 101, 118, 101, 114, 115, 105, 110, 103, 256, 104, 105, 115, 32, 112, 114, 111, 99, 101, 115, 115, 44, 32, 113, 117, 101, 114, 121, 105, 110, 103, 32, 107, 110, 111, 119, 110, 32, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 256, 101, 114, 109, 115, 32, 97, 103, 97, 105, 110, 115, 116, 256, 104, 101, 105, 114, 32, 99, 111, 114, 114, 101, 115, 112, 111, 110, 100, 105, 110, 103, 32, 100, 101, 110, 111, 116, 101, 100, 32, 115, 101, 113, 117, 101, 110, 99, 101, 44, 32, 117, 115, 105, 110, 103, 32, 97, 32, 108, 111, 111, 107, 117, 112, 256, 97, 98, 108, 101, 46, 32, 73, 110, 256, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 112, 97, 112, 101, 114, 44, 256, 104, 105, 115, 32, 108, 111, 111, 107, 117, 112, 256, 97, 98, 108, 101, 32, 105, 115, 32, 101, 110, 99, 111, 100, 101, 100, 32, 97, 110, 100, 32, 115, 116, 111, 114, 101, 100, 32, 97, 108, 111, 110, 103, 115, 105, 100, 101, 256, 104, 101, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 256, 101, 120, 116, 46]\n",
      "\n",
      "The new token size is 494\n"
     ]
    }
   ],
   "source": [
    "print(f\"After one iteration, we got our new list of tokens: {new_tokens}\\n\")\n",
    "print(f\"The new token size is {len(new_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6cf48",
   "metadata": {
    "papermill": {
     "duration": 0.014662,
     "end_time": "2024-04-25T20:55:53.736445",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.721783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can notice that with one iteration of BPE, we reduced our token sizes from 509 to 494. This makes sense because we have merge rule (32, 116) -> 256 which have appearred 15 times. \n",
    "\n",
    "**Note** \n",
    "\n",
    "In the previous three steps, we have seen how BBPE works in slow motion. Hopefully you can see why BBPE tokenizer is a good choice for many LLMs now. Here are some questions I had prior and I want to make sure they are clear for anyone who might still have confusions:\n",
    "\n",
    "1. Why BBPE tokenizer is able to recognize all characters? Because all characters will be encoded into raw bytes using UTF-8 first. Even the tokenizer has never encoutered a character in the training set, it can still encode it with the raw bytes. The character could be represented with multiple bytes, but it can still be encoded. \n",
    "\n",
    "2. Why BBPE only have 256 vocabulary size? BBPE tokenizer has a flexible vocabulary size, but its base vocabulary size is 256 because a raw byte consists of eight bits, and there are total 2^8 possibilities of one byte. However, when the BBPE tokenizer is trained, as new merge rules created, the vocabulary size will grow. \n",
    "\n",
    "3. Why BBPE tokenizer is a more efficient choice comparing to using Unicode? As we have shown above, because of the merge rules, we can compress a combination of tokens by replacing them with one new token. So we will have a shorter token sequence to represent the same text compared to using Unicode naively. \n",
    "\n",
    "In the next section, we will combine the steps above and train our own BBPE tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137833c1",
   "metadata": {
    "papermill": {
     "duration": 0.014599,
     "end_time": "2024-04-25T20:55:53.766353",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.751754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. Train a BBPE tokenizer with all steps above <a class=\"anchor\"  id=\"section_2_2_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "113ad1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.860505Z",
     "iopub.status.busy": "2024-04-25T20:55:53.860103Z",
     "iopub.status.idle": "2024-04-25T20:55:53.870453Z",
     "shell.execute_reply": "2024-04-25T20:55:53.869085Z"
    },
    "papermill": {
     "duration": 0.029858,
     "end_time": "2024-04-25T20:55:53.873021",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.843163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_pair_freq(tokens):\n",
    "    count = {}\n",
    "    for i in zip(tokens, tokens[1:]):\n",
    "        if i not in count.keys():\n",
    "            count[i] = 1\n",
    "        else:\n",
    "            count[i] += 1\n",
    "\n",
    "    # sort the pair counts to find the most frequent pair\n",
    "    sorted_pair_counts = {k: v for k, v in sorted(count.items(), key = lambda item: item[1], reverse = True)}\n",
    "    return sorted_pair_counts\n",
    "\n",
    "def merge(merge_pair, tokens, new_id):\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        # if we are at the last index, we will break out the loop to aviod out of index error\n",
    "        # then we will match current pair with the merge pair\n",
    "        if (i < len(tokens) - 1) and (tokens[i] == merge_pair[0]) and (tokens[i+1] == merge_pair[1]):\n",
    "            new_tokens.append(new_id)\n",
    "            # if we found a merge pair, add 2 to i to skip next value in list\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4356ce09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.906622Z",
     "iopub.status.busy": "2024-04-25T20:55:53.906242Z",
     "iopub.status.idle": "2024-04-25T20:55:53.914477Z",
     "shell.execute_reply": "2024-04-25T20:55:53.912814Z"
    },
    "papermill": {
     "duration": 0.028481,
     "end_time": "2024-04-25T20:55:53.917136",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.888655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the intial token size before any compression is 1520\n"
     ]
    }
   ],
   "source": [
    "# as mentioned, with BBPE, we can adjust the size of the final vocabulary by defining the number of merges\n",
    "given_texts = \"Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial 'tokens'). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256. The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for tokenization because it has low computational overhead and remains consistent and reliable.\"\n",
    "tokens = given_texts.encode('utf-8')\n",
    "# convert the binary tokens to a list of integers for each visualization\n",
    "tokens = list(map(int, tokens))\n",
    "print(f\"the intial token size before any compression is {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863851fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:53.949664Z",
     "iopub.status.busy": "2024-04-25T20:55:53.948826Z",
     "iopub.status.idle": "2024-04-25T20:55:53.997766Z",
     "shell.execute_reply": "2024-04-25T20:55:53.996192Z"
    },
    "papermill": {
     "duration": 0.068241,
     "end_time": "2024-04-25T20:55:54.000502",
     "exception": false,
     "start_time": "2024-04-25T20:55:53.932261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge pair (101, 32) and replace with new id 256\n",
      "Merge pair (115, 32) and replace with new id 257\n",
      "Merge pair (105, 110) and replace with new id 258\n",
      "Merge pair (116, 104) and replace with new id 259\n",
      "Merge pair (32, 97) and replace with new id 260\n",
      "Merge pair (101, 110) and replace with new id 261\n",
      "Merge pair (116, 32) and replace with new id 262\n",
      "Merge pair (100, 32) and replace with new id 263\n",
      "Merge pair (111, 114) and replace with new id 264\n",
      "Merge pair (259, 256) and replace with new id 265\n",
      "Merge pair (116, 111) and replace with new id 266\n",
      "Merge pair (97, 114) and replace with new id 267\n",
      "Merge pair (114, 101) and replace with new id 268\n",
      "Merge pair (97, 108) and replace with new id 269\n",
      "Merge pair (105, 257) and replace with new id 270\n",
      "Merge pair (116, 101) and replace with new id 271\n",
      "Merge pair (116, 105) and replace with new id 272\n",
      "Merge pair (99, 111) and replace with new id 273\n",
      "Merge pair (111, 102) and replace with new id 274\n",
      "Merge pair (116, 97) and replace with new id 275\n",
      "Merge pair (97, 99) and replace with new id 276\n",
      "Merge pair (32, 265) and replace with new id 277\n",
      "Merge pair (258, 103) and replace with new id 278\n",
      "Merge pair (109, 32) and replace with new id 279\n",
      "\n",
      "With 24 merge rules, we obtain new tokens size of 1109\n"
     ]
    }
   ],
   "source": [
    "# perform BBPE \n",
    "desired_vocab_size = 280\n",
    "num_of_merges = desired_vocab_size - 256\n",
    "# we need to create a dictionary to store all merge rule, it will be used for decoding purpose\n",
    "merge_rule = {}\n",
    "\n",
    "for i in range(num_of_merges):\n",
    "    pair_counts = count_pair_freq(tokens)\n",
    "    # we will take the highest rank pair as our merge pair\n",
    "    merge_pair = list(pair_counts.keys())[0]\n",
    "    new_id = 256 + i\n",
    "    merge_rule[merge_pair] = new_id\n",
    "    print(f\"Merge pair {merge_pair} and replace with new id {new_id}\")\n",
    "    tokens = merge(merge_pair, tokens, new_id)\n",
    "\n",
    "print(f\"\\nWith {len(merge_rule)} merge rules, we obtain new tokens size of {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "477f59e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:54.035451Z",
     "iopub.status.busy": "2024-04-25T20:55:54.035022Z",
     "iopub.status.idle": "2024-04-25T20:55:54.043194Z",
     "shell.execute_reply": "2024-04-25T20:55:54.041872Z"
    },
    "papermill": {
     "duration": 0.029509,
     "end_time": "2024-04-25T20:55:54.046242",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.016733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's take a look at the obtained vocabulary: {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b's ', 258: b'in', 259: b'th', 260: b' a', 261: b'en', 262: b't ', 263: b'd ', 264: b'or', 265: b'the ', 266: b'to', 267: b'ar', 268: b're', 269: b'al', 270: b'is ', 271: b'te', 272: b'ti', 273: b'co', 274: b'of', 275: b'ta', 276: b'ac', 277: b' the ', 278: b'ing', 279: b'm '}\n"
     ]
    }
   ],
   "source": [
    "# first, let's create the vocabulary list: it contains two part: base vocabulary and the merged rules\n",
    "# construct base vocab first, which are the raw bytes of each integer from 0 to 255\n",
    "vocabulary = {integer: bytes([integer]) for integer in range(256)}\n",
    "\n",
    "# then, we will add all merge rules into the vocabulary\n",
    "for k, v in merge_rule.items():\n",
    "    # remember merge rule item has this format (token1, token2): new_token\n",
    "    # and token1 and token2 are two integers, and new_token is a new integer outside of (0, 255)\n",
    "    # we will add the rule into vocabulary in this format: new_token: bytes(token1) + bytes(token2)\n",
    "    # since bytes(token1) and bytes(token2) are already stored in the dictionary, we can add the merge rule like follow:\n",
    "    vocabulary[v] = vocabulary[k[0]] + vocabulary[k[1]]\n",
    "\n",
    "print(f\"Let's take a look at the obtained vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da66c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:20:50.764595Z",
     "iopub.status.busy": "2024-04-23T20:20:50.764146Z",
     "iopub.status.idle": "2024-04-23T20:20:50.771111Z",
     "shell.execute_reply": "2024-04-23T20:20:50.769885Z",
     "shell.execute_reply.started": "2024-04-23T20:20:50.764545Z"
    },
    "papermill": {
     "duration": 0.017455,
     "end_time": "2024-04-25T20:55:54.079817",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.062362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5. Encoding and Decoding <a class=\"anchor\"  id=\"section_2_2_5\"></a>\n",
    "\n",
    "In step 4, we have trained our BBPE tokenizer and obtained our new vocabulary and merge rules. With those two dicitonary, it can be used to encode and decode between raw unicode bytes and tokens. In step 5, we will create encode and decode function to complete the functionality of this tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f016b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:54.113560Z",
     "iopub.status.busy": "2024-04-25T20:55:54.113147Z",
     "iopub.status.idle": "2024-04-25T20:55:54.119994Z",
     "shell.execute_reply": "2024-04-25T20:55:54.118400Z"
    },
    "papermill": {
     "duration": 0.027173,
     "end_time": "2024-04-25T20:55:54.122721",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.095548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder(tokens, vocabulary):\n",
    "    # this function will decode a list of integer tokens back to string\n",
    "    raw_bytes = b\"\".join(vocabulary[token] for token in tokens)\n",
    "    texts = raw_bytes.decode('utf-8')\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1136bdce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:54.156264Z",
     "iopub.status.busy": "2024-04-25T20:55:54.155785Z",
     "iopub.status.idle": "2024-04-25T20:55:54.163113Z",
     "shell.execute_reply": "2024-04-25T20:55:54.161869Z"
    },
    "papermill": {
     "duration": 0.027263,
     "end_time": "2024-04-25T20:55:54.165883",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.138620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoder(text, merge_rule):\n",
    "    # this function will encode a string into a list of tokens\n",
    "    # first, use utf8 to encode the text into raw bytes\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    # second, based on merge rules to merge raw bytes\n",
    "    # A good tip learned from the video: we can reuse the pair count function to get all possible pairs\n",
    "    # then we can check if any of those pairs are in the merge rules to be more efficient\n",
    "    pair_counts = count_pair_freq(tokens)\n",
    "    # go through the rules from the first to last\n",
    "    for merge_pair, new_id in merge_rule.items():\n",
    "        # when we found the qualified pairs, then perform merge\n",
    "        if merge_pair in pair_counts.keys():\n",
    "            tokens = merge(merge_pair, tokens, new_id)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b57516af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:54.200231Z",
     "iopub.status.busy": "2024-04-25T20:55:54.199813Z",
     "iopub.status.idle": "2024-04-25T20:55:54.227586Z",
     "shell.execute_reply": "2024-04-25T20:55:54.226649Z"
    },
    "papermill": {
     "duration": 0.047873,
     "end_time": "2024-04-25T20:55:54.230149",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.182276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the initial texts provided: \n",
      "Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial 'tokens'). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256. The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for tokenization because it has low computational overhead and remains consistent and reliable.\n",
      "---\n",
      "here is the returned texts after running encoder and then decoder: \n",
      "Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial 'tokens'). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256. The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for tokenization because it has low computational overhead and remains consistent and reliable.\n"
     ]
    }
   ],
   "source": [
    "# to test the encoder and decoder, we can provide a text, use encoder and decoder together and we should get the iinitial text back\n",
    "print(f\"here is the initial texts provided: \\n{given_texts}\")\n",
    "print('---')\n",
    "\n",
    "returned_texts = decoder(encoder(given_texts, merge_rule), vocabulary)\n",
    "print(f\"here is the returned texts after running encoder and then decoder: \\n{returned_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec4d2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T20:55:54.263693Z",
     "iopub.status.busy": "2024-04-25T20:55:54.263304Z",
     "iopub.status.idle": "2024-04-25T20:55:54.272210Z",
     "shell.execute_reply": "2024-04-25T20:55:54.270968Z"
    },
    "papermill": {
     "duration": 0.028522,
     "end_time": "2024-04-25T20:55:54.274679",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.246157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_texts == returned_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d484846",
   "metadata": {
    "papermill": {
     "duration": 0.015613,
     "end_time": "2024-04-25T20:55:54.306526",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.290913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Great! Now we have trained our own BBPE tokenizer, and also built a encoder and decoder function for the trained tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a0af7",
   "metadata": {
    "papermill": {
     "duration": 0.015489,
     "end_time": "2024-04-25T20:55:54.337768",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.322279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion <a class=\"anchor\"  id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348d243",
   "metadata": {
    "papermill": {
     "duration": 0.016827,
     "end_time": "2024-04-25T20:55:54.370403",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.353576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this post, we have go through the BBPE tokenizer with many details. Let's quickly recap what we have done so far:\n",
    "\n",
    "1. In order to understand why BBPE tokenizer is a popular choice for LLMs, we have checked other types of tokenizer, such as word-level tokenizer, character-level tokenizer, or byte level using UTF-8 directly. All of those tokenizers have its own advantage, but when using in the LLMs task, they all lack some key factors. \n",
    "\n",
    "2. On the other hand, BBPE tokenizer starts with a very small base vocabulary, but also provide the flexibility to add new vocabulary by merging frequently appearred pairs. So the LLMs research can find the good balance between good size of vocabulary and good size of encoded tokens. This is the one advantage BBPE tokenizer can provide while other methods lack. \n",
    "\n",
    "3. Then, we went through the initial BPE algorithm, and the byte-level implementation steps by steps. To train a BBPE tokenizer, all training texts will be encoded into raw bytes first (normally in UTF-8). Next, we will perform BPE algorithm to find the most frequently appearred pairs to merge. We will repeat this step until we reached the desired vocabulary size. Finally, we will create final vocabulary dictionay and all merge rules. Those two things will be stored for future encode and decode tasks. \n",
    "\n",
    "If you enjoy reading this and find it helpful, please give me a upvote. If you have any suggesstion or questions, please leave a comment. Thank you so much for reading!\n",
    "\n",
    "**Special Thanks to Andrej Karpathy's amazing work!**\n",
    "\n",
    "If you have time and want to know more about BBPE, please watch Andrej Karpathy's video. He went even deeper on this topic and shared many thoughts and addtional information. The video link is shared below:\n",
    "\n",
    "[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE&t=108s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e8851",
   "metadata": {
    "papermill": {
     "duration": 0.015697,
     "end_time": "2024-04-25T20:55:54.402677",
     "exception": false,
     "start_time": "2024-04-25T20:55:54.386980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.526317,
   "end_time": "2024-04-25T20:55:55.541977",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-25T20:55:42.015660",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
